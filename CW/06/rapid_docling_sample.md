## 一、摘要

大型語言模型(LargeLanguageModels)在各種领域中的許多應用上靳露頭角，逐渐 成為普羅大日常生活的一部分。然而，LLM的幻覺問題（Hallucination)會造成真假 廣。傅統的資訊索（InformationRetrieval）然在基於關鍵字的搜荨中表現出色'能 相關資訊。不只是如此，使用者仍需手動搜出的文章中定位所需的段落或片段， 亚進一步對查詢结果進行總結舆解耦。本研究旨在透過改良版的索增強生成（Retriev alAugmentedGeneration），使大型語言模型的回答内容有理有據，提高使用者對大型語 言模型回覆内容的信心程度。

## 二、研究動機與研究問題

## （一）、研究動機

自ChatGPT問世以來'大型語言模型（LargeLanguageModels'以下簡L LM)迅速地崛起走進公舆企業的視野，在聊天對話、問題回應、翻等各 式自然語言任務上達到出的表現，部分經過特别訓的LLM甚至在專家考 試、多模態理解與整合等複雜任務中表現出人的確性，在可以预见的未來 將對各行各業產生深遠的影響。

然而，在這些令人艷的表現背後，有些問題逐浮上面，其中幻覺問 題（Hallucination)是一大。当LLM在生成回覆時，由於亚不涉及搜寻與 判資訊正確性的段，因此有可能會回答看似語意通顺、辑清晰，實则子 虚烏有、錯百出的回應，當使用者無法辨别真假時，可能會造成嚴重的後果。

為了改善幻覺問題，索增強生成（RetrievalAugmentedGeneration'以下簡 RAG)被提出。透過給予LLM含有正確答案的文本，藉此提高回覆内容的 正確性，有效降低了產生幻覺的可能性。然而，獲取文本的過程、添加進LLM 的文本數量與长度等，也將影響RAG的運作效能。若獲取的文本带有錯的 資訓，或是因為過長的参考資料造成過多冗餘資訊，都將對LLM回答的品質 造成負面影響。本研究旨在透過改良RAG框架的運作流程來解决上述問題。

## （二）、研究問題

透過理解大型語言模型的運作原理、幻覺問題可能的成因與RAG的優勢後， 我們决定藉由改良RAG的連作流程來進一步提升這個框架在特定問題上的表現。 具體而言，本研究的問題如下：

- 1.透過新的前處理框架，改善過長参考文本所衍伸出的問題
- 2.结合预訓模型與既有的工具，以最小成本建置一個可供驗證的本
- 3.設計前後端介面，降低使用者的操作门楹

## （一）、實驗結果

以下是使用all-MiniLM-L6-v2作為Embeddingmodel，在不同問題下，更改不同 Top-K得出的實驗結果。

|                  |   Top-K |   Precision |   AP |   NDCG |
|------------------|---------|-------------|------|--------|
| Covid-19 Wiki Q1 |       5 |        1    | 1    |   1    |
| Covid-19 Wiki Q1 |      10 |        0.9  | 0.96 |   0.93 |
| Covid-19 Wiki Q1 |      20 |        0.65 | 0.93 |   0.75 |
| Covid-19 Wiki Q2 |       5 |        0.6  | 0.92 |   0.7  |
| Covid-19 Wiki Q2 |      10 |        0.3  | 0.92 |   0.45 |
| Covid-19 Wiki Q2 |      20 |        0.15 | 0.92 |   0.3  |
| Covid-19 Wiki Q3 |       5 |        0.8  | 1    |   0.87 |
| Covid-19 Wiki Q3 |      10 |        0.5  | 0.91 |   0.63 |
| Covid-19 Wiki Q3 |      20 |        0.3  | 0.81 |   0.44 |

Q1 : What measures can people take to prevent COVID-19 while they are outside?

Q2 : What is the name of the virus that causes COVID-19?

Q3 : How can I tell if I have COVID-19?

CG指標上均表現出较高的分數，顯示出良好的效能。然而，當K值增加至20時， 少。因此，選Top-5或Top-10的回應已能滿足需求，增加回應數量亚未顯著提升 回答品質，反而可能會對系統效率產生負面影響。

以下是使用all-MiniLM-L6-v2作為Embeddingmodel'在不同問題下，加上自 動辨識版本後，更改不同Top-K得出的實驗結果。

|                 |   Top-K |   Precision |   AP |   NDCG |
|-----------------|---------|-------------|------|--------|
| Linux Update Q1 |       5 |         1   | 1    |   1    |
| Linux Update Q1 |      10 |         1   | 1    |   1    |
| Linux Update Q1 |      20 |         1   | 1    |   1    |
| Linux Update Q2 |       5 |         1   | 1    |   1    |
| Linux Update Q2 |      10 |         0.8 | 0.92 |   0.85 |
| Linux Update Q2 |      20 |         0.6 | 0.85 |   0.69 |
| Linux Update Q3 |       5 |         1   | 1    |   1    |
| Linux Update Q3 |      10 |         1   | 1    |   1    |
| Linux Update Q3 |      20 |         1   | 1    |   1    |

## 五、結果與讨