import os
import requests
import pandas as pd
from typing import List, Tuple, Dict, Optional
from tqdm import tqdm
import uuid
from qdrant_client import QdrantClient, models
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from openai import OpenAI

from deepeval.models import DeepEvalBaseLLM
from deepeval.metrics import (
    FaithfulnessMetric,
    AnswerRelevancyMetric,
    ContextualRecallMetric,
    ContextualPrecisionMetric,
    ContextualRelevancyMetric
)
from deepeval.test_case import LLMTestCase

# ============================================================================
# é…ç½®åƒæ•¸
# ============================================================================

class Config:
    """é…ç½®é¡åˆ¥"""
    # Qdrant
    QDRANT_URL = "http://localhost:6333"
    COLLECTION_NAME = "day6_water_company_kb"
    
    # LLM API
    LLM_BASE_URL = "https://ws-06.huannago.com/v1"
    LLM_MODEL = "gemma-3-27b-it"
    LLM_TEMPERATURE = 0.3
    
    # Embedding API
    EMBEDDING_URL = "https://ws-04.wade0426.me/embed"
    EMBEDDING_DIM = 4096
    
    # Reranker æ¨¡å‹è·¯å¾‘ (æœ¬åœ°)
    RERANKER_MODEL_PATH = os.path.expanduser("./Models/Qwen3-Reranker-0.6B")
    
    # æª”æ¡ˆè·¯å¾‘
    QA_DATA_PATH = "./day6-c/qa_data.txt"
    QUESTIONS_CSV = "./day6-c/day6_HW_questions.csv"
    OUTPUT_CSV = "./day6-c/outputs/day6_HW_questions_completed.csv"
    
    # æª¢ç´¢åƒæ•¸
    INITIAL_SEARCH_LIMIT = 5  # åˆå§‹æª¢ç´¢æ•¸é‡
    RERANK_TOP_K = 3  # Rerank å¾Œä¿ç•™çš„æ–‡ä»¶æ•¸é‡
    
    # DeepEval åƒæ•¸
    DEEPEVAL_THRESHOLD = 0.5

# ============================================================================
# Embedding API
# ============================================================================

def get_embeddings(texts: List[str]) -> List[List[float]]:

    response = requests.post(
        Config.EMBEDDING_URL,
        json={
            "texts": texts,
        "normalize": True,
        "batch_size": 32
        }
    )
    
    return response.json()['embeddings']

# ============================================================================
# Qdrant å‘é‡è³‡æ–™åº«
# ============================================================================

class QdrantManager:
    """Qdrant ç®¡ç†é¡åˆ¥"""
    
    def __init__(self):
        self.client = QdrantClient(url=Config.QDRANT_URL)
        self.collection_name = Config.COLLECTION_NAME
    
    def create_collection(self):
        """å»ºç«‹æ”¯æ´ Hybrid Search çš„é›†åˆ"""
        try:
            # æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collections = self.client.get_collections().collections
            if any(col.name == self.collection_name for col in collections):
                print(f"âœ… é›†åˆ '{self.collection_name}' å·²å­˜åœ¨ï¼Œåˆªé™¤èˆŠçš„...")
                self.client.delete_collection(self.collection_name)
            
            # å»ºç«‹æ–°é›†åˆ
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config={
                    "dense": models.VectorParams(
                        distance=models.Distance.COSINE,
                        size=Config.EMBEDDING_DIM,
                    ),
                },
                sparse_vectors_config={
                    "sparse": models.SparseVectorParams(
                        modifier=models.Modifier.IDF
                    )
                },
            )
            print(f"âœ… æˆåŠŸå»ºç«‹é›†åˆ '{self.collection_name}'")
        except Exception as e:
            print(f"âŒ å»ºç«‹é›†åˆå¤±æ•—: {e}")
            raise
    
    def load_documents(self, qa_data_path: str):
        """è¼‰å…¥çŸ¥è­˜åº«"""
        print(f"\nè¼‰å…¥çŸ¥è­˜åº«: {qa_data_path}")
        
        # è®€å–æ–‡ä»¶
        with open(qa_data_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŒ‰å•ç­”å°åˆ†å‰²
        chunks = []
        current_chunk = []
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            if line.startswith('ä¾†æºï¼š'):
                current_chunk.append(line)
                if current_chunk:
                    chunk_text = '\n'.join(current_chunk)
                    if len(chunk_text) > 20:  # éæ¿¾å¤ªçŸ­çš„
                        chunks.append(chunk_text)
                current_chunk = []
            elif line:
                current_chunk.append(line)
        
        # åŠ å…¥æœ€å¾Œä¸€å€‹ chunk
        if current_chunk:
            chunk_text = '\n'.join(current_chunk)
            if len(chunk_text) > 20:
                chunks.append(chunk_text)
        
        print(f"âœ“ åˆ†å‰²ç‚º {len(chunks)} å€‹æ–‡æª”ç‰‡æ®µ")
        
        # ç”ŸæˆåµŒå…¥
        print("æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡...")
        embeddings = get_embeddings(chunks)
        
        # å»ºç«‹ç´¢å¼•
        points = []
        for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            points.append(models.PointStruct(
                id=uuid.uuid4().hex,
                vector={
                    "dense": embedding,
                    "sparse": models.Document(
                        text=chunk,
                        model="Qdrant/bm25",
                    ),
                },
                payload={
                    "text": chunk,
                    "chunk_id": idx + 1
                }
            ))
        
        # æ’å…¥å‘é‡è³‡æ–™åº«
        self.client.upsert(
            collection_name=Config.COLLECTION_NAME,
            points=points
        )
        print(f"âœ“ æˆåŠŸæ’å…¥ {len(points)} å€‹æ–‡æª”ç‰‡æ®µåˆ°å‘é‡è³‡æ–™åº«")
    
    def hybrid_search(self, query: str, limit: int = Config.INITIAL_SEARCH_LIMIT) -> List[str]:
        """
        Hybrid Search (Dense + Sparse/BM25)
        
        Args:
            query: æŸ¥è©¢å­—ä¸²
            limit: æª¢ç´¢æ•¸é‡
        
        Returns:
            æ–‡ä»¶åˆ—è¡¨
        """
        try:
            # å–å¾— query embedding
            query_embedding = get_embeddings([query])[0]
            
            # Hybrid Search with RRF
            response = self.client.query_points(
                collection_name=self.collection_name,
                prefetch=[
                    # BM25 é—œéµå­—æœç´¢
                    models.Prefetch(
                        query=models.Document(
                            text=query,
                            model="Qdrant/bm25",
                        ),
                        using="sparse",
                        limit=limit,
                    ),
                    # èªç¾©æœç´¢
                    models.Prefetch(
                        query=query_embedding,
                        using="dense",
                        limit=limit,
                    ),
                ],
                # ä½¿ç”¨ RRF èåˆæ¼”ç®—æ³•
                query=models.FusionQuery(fusion=models.Fusion.RRF),
                limit=limit,
            )
            
            # æå–æ–‡ä»¶
            documents = [point.payload["text"] for point in response.points]
            return documents
            
        except Exception as e:
            print(f"âŒ Hybrid Search å¤±æ•—: {e}")
            raise

# ============================================================================
# Reranker
# ============================================================================

class Reranker:
    """Reranker é¡åˆ¥ (Qwen3-Reranker-0.6B)"""
    
    def __init__(self):
        print("ğŸ”„ è¼‰å…¥ Reranker æ¨¡å‹...")
        
        # è¼‰å…¥æ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(
            Config.RERANKER_MODEL_PATH,
            local_files_only=True,
            trust_remote_code=True,
            padding_side='left'
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            Config.RERANKER_MODEL_PATH,
            local_files_only=True,
            trust_remote_code=True
        ).eval()
        
        # é…ç½®åƒæ•¸
        self.token_false_id = self.tokenizer.convert_tokens_to_ids("no")
        self.token_true_id = self.tokenizer.convert_tokens_to_ids("yes")
        self.max_length = 8192
        
        # Prompt æ¨¡æ¿
        self.prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
        self.suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
        self.prefix_tokens = self.tokenizer.encode(self.prefix, add_special_tokens=False)
        self.suffix_tokens = self.tokenizer.encode(self.suffix, add_special_tokens=False)
        
        print("âœ… Reranker æ¨¡å‹è¼‰å…¥å®Œæˆ")
    
    def format_instruction(self, instruction: str, query: str, doc: str) -> str:
        """æ ¼å¼åŒ– reranker è¼¸å…¥"""
        if instruction is None:
            instruction = 'Given a web search query, retrieve relevant passages that answer the query'
        return f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}"
    
    def process_inputs(self, pairs: List[str]):
        """è™•ç†è¼¸å…¥"""
        inputs = self.tokenizer(
            pairs, 
            padding=False, 
            truncation='longest_first',
            return_attention_mask=False, 
            max_length=self.max_length - len(self.prefix_tokens) - len(self.suffix_tokens)
        )
        
        for i, ele in enumerate(inputs['input_ids']):
            inputs['input_ids'][i] = self.prefix_tokens + ele + self.suffix_tokens
        
        inputs = self.tokenizer.pad(
            inputs, 
            padding=True, 
            return_tensors="pt", 
            max_length=self.max_length
        )
        
        for key in inputs:
            inputs[key] = inputs[key].to(self.model.device)
        
        return inputs
    
    @torch.no_grad()
    def compute_scores(self, inputs):
        """è¨ˆç®—ç›¸é—œåº¦åˆ†æ•¸"""
        batch_scores = self.model(**inputs).logits[:, -1, :]
        true_vector = batch_scores[:, self.token_true_id]
        false_vector = batch_scores[:, self.token_false_id]
        batch_scores = torch.stack([false_vector, true_vector], dim=1)
        batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)
        scores = batch_scores[:, 1].exp().tolist()
        return scores
    
    def rerank(self, query: str, documents: List[str], 
               task_instruction: str = None, top_k: int = Config.RERANK_TOP_K) -> List[Tuple[str, float]]:
        """
        é‡æ–°æ’åºæ–‡ä»¶
        
        Args:
            query: æŸ¥è©¢å­—ä¸²
            documents: æ–‡ä»¶åˆ—è¡¨
            task_instruction: ä»»å‹™æŒ‡ä»¤
            top_k: è¿”å›å‰ k å€‹çµæœ
        
        Returns:
            (æ–‡ä»¶, åˆ†æ•¸) å…ƒçµ„åˆ—è¡¨
        """
        if not documents:
            return []
        
        if task_instruction is None:
            task_instruction = 'æ ¹æ“šä½¿ç”¨è€…å•é¡Œï¼Œæ‰¾å‡ºæœ€ç›¸é—œçš„å°æ°´å…¬å¸å®¢æœè³‡è¨Š'
        
        # æ ¼å¼åŒ–è¼¸å…¥
        pairs = [self.format_instruction(task_instruction, query, doc) for doc in documents]
        
        # è¨ˆç®—åˆ†æ•¸
        inputs = self.process_inputs(pairs)
        scores = self.compute_scores(inputs)
        
        # çµ„åˆä¸¦æ’åº
        doc_scores = list(zip(documents, scores))
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        return doc_scores[:top_k]

# ============================================================================
# LLM (ç”¨æ–¼ç”Ÿæˆç­”æ¡ˆå’Œ DeepEval)
# ============================================================================

class CustomLLM(DeepEvalBaseLLM):
    """è‡ªè¨‚ LLM é¡åˆ¥ (ç”¨æ–¼ DeepEval)"""
    
    def __init__(self):
        self.client = OpenAI(
            api_key="NoNeed",
            base_url="https://ws-02.wade0426.me/v1"
        )
        self.model_name = "local-model"
    
    def load_model(self):
        return self.client
    
    def generate(self, prompt: str) -> str:
        """ç”Ÿæˆå›æ‡‰"""
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=Config.LLM_TEMPERATURE,
        )
        return response.choices[0].message.content
    
    async def a_generate(self, prompt: str) -> str:
        return self.generate(prompt)
    
    def get_model_name(self):
        return self.model_name

class RAGSystem:
    """RAG ç³»çµ±"""
    
    def __init__(self, qdrant_manager: QdrantManager, reranker: Reranker):
        self.qdrant = qdrant_manager
        self.reranker = reranker
        self.llm = CustomLLM()
    
    def query_rewrite(self, query: str) -> str:
        """Query Rewrite - å°‡ä½¿ç”¨è€…å•é¡Œæ”¹å¯«ç‚ºæ›´é©åˆæª¢ç´¢çš„å½¢å¼"""
        prompt = f"""è«‹å°‡ä»¥ä¸‹ä½¿ç”¨è€…å•é¡Œæ”¹å¯«ç‚ºæ›´é©åˆæª¢ç´¢çš„æŸ¥è©¢èªå¥ã€‚
        ä¿æŒå•é¡Œçš„æ ¸å¿ƒæ„åœ–ï¼Œä½†ä½¿ç”¨æ›´ç²¾ç¢ºçš„é—œéµè©ã€‚

        åŸå§‹å•é¡Œï¼š{query}

        æ”¹å¯«å¾Œçš„æŸ¥è©¢ï¼ˆåªè¼¸å‡ºæ”¹å¯«çµæœï¼Œä¸è¦å…¶ä»–èªªæ˜ï¼‰ï¼š"""
        
        rewritten_query = self.llm.generate(prompt)
        return rewritten_query.strip()
    
    def retrieve_documents(self, query: str, use_rewrite: bool = True) -> Tuple[List[str], str]:
        """
        æª¢ç´¢ç›¸é—œæ–‡ä»¶
        
        Args:
            query: åŸå§‹æŸ¥è©¢
            use_rewrite: æ˜¯å¦ä½¿ç”¨ query rewrite
        
        Returns:
            (æª¢ç´¢åˆ°çš„æ–‡ä»¶åˆ—è¡¨, å¯¦éš›ä½¿ç”¨çš„æŸ¥è©¢)
        """
        # Query Rewrite
        if use_rewrite:
            search_query = self.query_rewrite(query)
            print(f"  ğŸ”„ Query Rewrite: {query} â†’ {search_query}")
        else:
            search_query = query
        
        # Hybrid Search
        print(f"  ğŸ” Hybrid Search...")
        candidate_docs = self.qdrant.hybrid_search(search_query, Config.INITIAL_SEARCH_LIMIT)
        print(f"  ğŸ“„ æ‰¾åˆ° {len(candidate_docs)} å€‹å€™é¸æ–‡ä»¶")
        
        # Rerank
        print(f"  ğŸ¯ Reranking...")
        reranked_results = self.reranker.rerank(search_query, candidate_docs, top_k=Config.RERANK_TOP_K)
        
        documents = [doc for doc, score in reranked_results]
        print(f"  âœ… æœ€çµ‚ä¿ç•™ {len(documents)} å€‹æ–‡ä»¶")
        
        return documents, search_query
    
    def generate_answer(self, query: str, context_docs: List[str]) -> str:
        """æ ¹æ“šæª¢ç´¢åˆ°çš„æ–‡ä»¶ç”Ÿæˆç­”æ¡ˆ"""
        context = "\n\n".join([f"[æ–‡ä»¶ {i+1}]\n{doc}" for i, doc in enumerate(context_docs)])
        
        prompt = f"""ä½ æ˜¯å°ç£è‡ªä¾†æ°´å…¬å¸çš„AIå®¢æœåŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„åƒè€ƒæ–‡ä»¶å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

        åƒè€ƒæ–‡ä»¶ï¼š
        {context}

        ä½¿ç”¨è€…å•é¡Œï¼š{query}

        1. è«‹æ ¹æ“šåƒè€ƒæ–‡ä»¶æä¾›æº–ç¢ºã€å®Œæ•´ä¸”æ˜“æ‡‚çš„å›ç­”ã€‚å¦‚æœåƒè€ƒæ–‡ä»¶ä¸­æ²’æœ‰è¶³å¤ è³‡è¨Šï¼Œè«‹èª å¯¦å‘ŠçŸ¥ã€‚
        2. ç­”æ¡ˆè«‹ç°¡æ½”ï¼Œå¯«æˆä¸€è¡Œå®Œæ•´å›ç­”

        å›ç­”ï¼š"""
        
        answer = self.llm.generate(prompt)
        return answer.strip()
    
    def answer_query(self, query: str) -> Dict[str, any]:
        """
        å®Œæ•´çš„å•ç­”æµç¨‹
        
        Returns:
            åŒ…å« query, answer, retrieval_contexts çš„å­—å…¸
        """
        print(f"\n{'='*80}")
        print(f"â“ ä½¿ç”¨è€…å•é¡Œ: {query}")
        
        # æª¢ç´¢æ–‡ä»¶
        documents, search_query = self.retrieve_documents(query)
        
        # ç”Ÿæˆç­”æ¡ˆ
        print(f"  ğŸ’¬ ç”Ÿæˆç­”æ¡ˆ...")
        answer = self.generate_answer(query, documents)
        
        print(f"  âœ… ç­”æ¡ˆ: {answer[:100]}...")
        
        return {
            "query": query,
            "answer": answer,
            "retrieval_contexts": documents,
            "search_query": search_query
        }

# ============================================================================
# DeepEval è©•ä¼°
# ============================================================================

def evaluate_with_deepeval(result: Dict, expected_answer: str, custom_llm: CustomLLM) -> Dict[str, float]:
    """
    ä½¿ç”¨ DeepEval è©•ä¼° RAG ç³»çµ±
    
    Args:
        result: RAG ç³»çµ±çš„è¼¸å‡ºçµæœ
        expected_answer: é æœŸç­”æ¡ˆ (ground truth)
        custom_llm: è‡ªè¨‚ LLM
    
    Returns:
        å„é …æŒ‡æ¨™çš„åˆ†æ•¸
    """
    print(f"\n  ğŸ“Š DeepEval è©•ä¼°ä¸­...")
    
    # å»ºç«‹æ¸¬è©¦æ¡ˆä¾‹
    test_case = LLMTestCase(
        input=result["query"],
        actual_output=result["answer"],
        expected_output=expected_answer,
        retrieval_context=result["retrieval_contexts"]
    )
    
    # å®šç¾©æŒ‡æ¨™
    metrics = {
        "Faithfulness": FaithfulnessMetric(
            threshold=Config.DEEPEVAL_THRESHOLD,
            model=custom_llm,
            include_reason=False
        ),
        "Answer_Relevancy": AnswerRelevancyMetric(
            threshold=Config.DEEPEVAL_THRESHOLD,
            model=custom_llm,
            include_reason=False
        ),
        "Contextual_Recall": ContextualRecallMetric(
            threshold=Config.DEEPEVAL_THRESHOLD,
            model=custom_llm,
            include_reason=False
        ),
        "Contextual_Precision": ContextualPrecisionMetric(
            threshold=Config.DEEPEVAL_THRESHOLD,
            model=custom_llm,
            include_reason=False
        ),
        "Contextual_Relevancy": ContextualRelevancyMetric(
            threshold=Config.DEEPEVAL_THRESHOLD,
            model=custom_llm,
            include_reason=False
        )
    }
    
    # è©•ä¼°å„é …æŒ‡æ¨™
    scores = {}
    for metric_name, metric in metrics.items():
        try:
            metric.measure(test_case)
            scores[metric_name] = metric.score
            print(f"    {metric_name}: {metric.score:.4f}")
        except Exception as e:
            print(f"    âš ï¸ {metric_name} è©•ä¼°å¤±æ•—: {e}")
            scores[metric_name] = None
    
    return scores

# ============================================================================
# ä¸»ç¨‹å¼
# ============================================================================

def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 80)
    print("å°æ°´å…¬å¸ AI å®¢æœåŠ©æ‰‹ - Day 6 ä½œæ¥­")
    print("=" * 80)
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    os.makedirs(os.path.dirname(Config.OUTPUT_CSV), exist_ok=True)
    
    # 1. åˆå§‹åŒ– Qdrant
    print("\nğŸ“¦ åˆå§‹åŒ– Qdrant...")
    qdrant_manager = QdrantManager()
    qdrant_manager.create_collection()
    qdrant_manager.load_documents(Config.QA_DATA_PATH)
    
    # 2. åˆå§‹åŒ– Reranker
    print("\nğŸ¯ åˆå§‹åŒ– Reranker...")
    reranker = Reranker()
    
    # 3. åˆå§‹åŒ– RAG ç³»çµ±
    print("\nğŸ¤– åˆå§‹åŒ– RAG ç³»çµ±...")
    rag_system = RAGSystem(qdrant_manager, reranker)
    
    # 4. è®€å–å•é¡Œè³‡æ–™
    print(f"\nğŸ“– è®€å–å•é¡Œè³‡æ–™: {Config.QUESTIONS_CSV}")
    df_questions = pd.read_excel(Config.QUESTIONS_CSV)
    
    # **åªè™•ç†å‰5ç­†**
    df_questions = df_questions.head(5)
    print(f"ğŸ“ è™•ç†å‰ {len(df_questions)} ç­†å•é¡Œ")
    
    # è®€å–åƒè€ƒç­”æ¡ˆ
    qa_answer_path = Config.QUESTIONS_CSV.replace("day6_HW_questions.csv", "questions_answer.csv")
    df_answers = pd.read_excel(qa_answer_path)
    
    # 5. è™•ç†æ¯å€‹å•é¡Œ
    results = []
    
    for idx, row in tqdm(df_questions.iterrows(), total=len(df_questions), desc="è™•ç†å•é¡Œ"):
        q_id = row['q_id']
        question = row['questions']
        
        # å–å¾—åƒè€ƒç­”æ¡ˆ
        expected_answer = df_answers[df_answers['q_id'] == q_id]['answer'].values[0]
        
        print(f"\n{'='*80}")
        print(f"è™•ç†å•é¡Œ {q_id}/{len(df_questions)}")
        
        try:
            # RAG å•ç­”
            result = rag_system.answer_query(question)
            
            # DeepEval è©•ä¼°
            scores = evaluate_with_deepeval(result, expected_answer, rag_system.llm)
            
            # è¨˜éŒ„çµæœ
            results.append({
                'q_id': q_id,
                'questions': question,
                'answer': result['answer'],
                'Faithfulness': scores.get('Faithfulness'),
                'Answer_Relevancy': scores.get('Answer_Relevancy'),
                'Contextual_Recall': scores.get('Contextual_Recall'),
                'Contextual_Precision': scores.get('Contextual_Precision'),
                'Contextual_Relevancy': scores.get('Contextual_Relevancy')
            })
            
        except Exception as e:
            print(f"âŒ è™•ç†å•é¡Œ {q_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            results.append({
                'q_id': q_id,
                'questions': question,
                'answer': "éŒ¯èª¤ï¼šç„¡æ³•ç”Ÿæˆç­”æ¡ˆ",
                'Faithfulness': None,
                'Answer_Relevancy': None,
                'Contextual_Recall': None,
                'Contextual_Precision': None,
                'Contextual_Relevancy': None
            })
    
    # 6. å„²å­˜çµæœ
    print(f"\nğŸ’¾ å„²å­˜çµæœåˆ°: {Config.OUTPUT_CSV}")
    df_results = pd.DataFrame(results)
    df_results.to_csv(Config.OUTPUT_CSV, index=False, encoding='utf-8-sig')
    
    # 7. é¡¯ç¤ºçµ±è¨ˆ
    print("\n" + "=" * 80)
    print("ğŸ“Š è©•ä¼°çµæœçµ±è¨ˆ")
    print("=" * 80)
    
    for metric in ['Faithfulness', 'Answer_Relevancy', 'Contextual_Recall', 
                   'Contextual_Precision', 'Contextual_Relevancy']:
        scores = df_results[metric].dropna()
        if len(scores) > 0:
            print(f"{metric:25s}: å¹³å‡ {scores.mean():.4f} | æœ€å° {scores.min():.4f} | æœ€å¤§ {scores.max():.4f}")
        else:
            print(f"{metric:25s}: ç„¡æœ‰æ•ˆæ•¸æ“š")
    
    print("\nâœ… å®Œæˆï¼")

if __name__ == "__main__":
    main()