from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import requests
from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter
from semantic_text_splitter import TextSplitter
import pandas as pd
import os
import re

# ==================== è¨­å®š ====================

client = QdrantClient(url="http://localhost:6333")

EMBED_API_URL = "https://ws-04.wade0426.me/embed"
SCORE_API_URL = "https://hw-01.wade0426.me/submit_answer"

# ğŸ”¥ æ¥µå¤§åˆ‡å¡Š - ç¢ºä¿å–®ä¸€å€å¡ŠåŒ…å«å®Œæ•´è³‡è¨Š
CHUNKING_PARAMS = {
    "fixed": {
        "chunk_size": 300,      
        "chunk_overlap": 10,    
        "separator": "ã€‚"
    },
    "sliding": {
        "chunk_size": 350,      
        "chunk_overlap": 15,    
        "separators": ["ã€‚", "\n\n", "\n"]
    },
    "semantic": {
        "min_size": 150,        
        "max_size": 1300         
    }
}

SEARCH_TOP_K = 10
BATCH_SIZE = 32
API_TIMEOUT = 60

# ==================== æ ¸å¿ƒå‡½æ•¸ ====================

def get_embedding(texts: list) -> tuple:
    """ç²å–æ–‡æœ¬åµŒå…¥å‘é‡"""
    data = {
        "texts": texts,
        "normalize": True,
        "batch_size": BATCH_SIZE
    }
    response = requests.post(EMBED_API_URL, json=data, timeout=API_TIMEOUT)
    return response.json()['embeddings'], response.json()['dimension']

def build_collection(client: QdrantClient, name: str, data: list[dict], dim: int):
    """å»ºç«‹ä¸¦å¡«å……é›†åˆ"""
    collections = [c.name for c in client.get_collections().collections]
    if name in collections:
        client.delete_collection(name)

    client.create_collection(
        collection_name=name,
        vectors_config=VectorParams(size=dim, distance=Distance.COSINE)
    )
    
    points = []
    total = len(data)
    print(f"   è™•ç† {total} å€‹å€å¡Š...")
    
    for i in range(0, total, BATCH_SIZE):
        batch = data[i:i+BATCH_SIZE]
        texts = [item["text"] for item in batch]
        
        try:
            embeddings, _ = get_embedding(texts)
            
            for j, (item, emb) in enumerate(zip(batch, embeddings)):
                if len(emb) == dim:
                    points.append(PointStruct(
                        id=i+j+1,
                        payload={"text": item["text"], "source": item["source"]},
                        vector=emb
                    ))
            
            print(f"   é€²åº¦: {min(i+BATCH_SIZE, total)}/{total}", end='\r')
        except Exception as e:
            print(f"\n   âš ï¸ æ‰¹æ¬¡ {i} å¤±æ•—: {e}")
            continue
    
    if points:
        client.upsert(collection_name=name, points=points)
        print(f"\n   âœ… å»ºç«‹é›†åˆ '{name}': {len(points)} å€‹å‘é‡")

def expand_query(question: str) -> list[str]:
    """
    æŸ¥è©¢æ“´å±• - ç”Ÿæˆå¤šå€‹æŸ¥è©¢è®Šé«”
    """
    queries = [question]
    
    # ç§»é™¤ç–‘å•è©
    for word in ['ä½•è¬‚', 'ä»€éº¼', 'ç‚ºä½•', 'å¦‚ä½•', 'å“ªäº›', '?', '?']:
        if word in question:
            q = question.replace(word, '').strip()
            if len(q) > 5:
                queries.append(q)
    
    # æå–é—œéµè©
    keywords = re.findall(r'[\u4e00-\u9fa5]{2,}', question)
    if keywords:
        sorted_kw = sorted(keywords, key=len, reverse=True)[:3]
        queries.append(' '.join(sorted_kw))
    
    return list(set(queries))[:4]

def search_multi_query(collection_name: str, queries: list[str], top_k: int = 20) -> list:
    """å¤šæŸ¥è©¢æœå°‹ä¸¦åˆä½µçµæœ"""
    all_results = {}
    
    for query in queries:
        try:
            query_vector, _ = get_embedding([query])
            
            search_result = client.query_points(
                collection_name=collection_name,
                query=query_vector[0],
                limit=top_k
            )
            
            for point in search_result.points:
                point_id = point.id
                if point_id not in all_results or point.score > all_results[point_id].score:
                    all_results[point_id] = point
        except:
            continue
    
    sorted_results = sorted(all_results.values(), key=lambda x: x.score, reverse=True)
    return sorted_results[:top_k]

def select_best_candidate(candidates: list, question: str) -> dict:
    """
    æ™ºèƒ½é¸æ“‡æœ€ä½³å€™é¸
    ç¶œåˆè€ƒæ…®: ç›¸ä¼¼åº¦ã€é•·åº¦ã€é—œéµè©åŒ¹é…
    """
    if not candidates:
        return None
    
    # æå–å•é¡Œé—œéµè©
    question_keywords = set(re.findall(r'[\u4e00-\u9fa5]{2,}', question))
    
    best_candidate = None
    best_score = -1
    
    for candidate in candidates:
        text = candidate.payload['text']
        similarity = candidate.score
        
        # ç¶œåˆè©•åˆ†
        score = 0
        
        # 1. ç›¸ä¼¼åº¦ (50%)
        score += similarity * 0.5
        
        # 2. é•·åº¦çå‹µ (30%)
        # 800-1500 å­—å…ƒæœ€ä½³
        length = len(text)
        if 800 <= length <= 1500:
            length_bonus = 0.3
        elif length > 1500:
            length_bonus = 0.3 * (1 - (length - 1500) / 1000)
        else:
            length_bonus = 0.3 * (length / 800)
        score += max(0, length_bonus)
        
        # 3. é—œéµè©åŒ¹é…åº¦ (20%)
        text_keywords = set(re.findall(r'[\u4e00-\u9fa5]{2,}', text))
        if question_keywords:
            keyword_overlap = len(question_keywords & text_keywords)
            keyword_ratio = keyword_overlap / len(question_keywords)
            score += keyword_ratio * 0.2
        
        if score > best_score:
            best_score = score
            best_candidate = candidate
    
    return best_candidate

def get_score(q_id: int, retrieve_text: str) -> float:
    """ä½¿ç”¨ API ç²å–è©•åˆ†"""
    try:
        payload = {
            "q_id": int(q_id),
            "student_answer": str(retrieve_text).strip()
        }
        
        response = requests.post(SCORE_API_URL, json=payload, timeout=API_TIMEOUT)
        
        if response.status_code == 200:
            return response.json().get('score', 0.0)
        else:
            print(f"   âš ï¸ API éŒ¯èª¤ {response.status_code}")
            return 0.0
    except Exception as e:
        print(f"   âš ï¸ è©•åˆ†ç•°å¸¸: {e}")
        return 0.0

# ==================== åˆ‡å¡Šæ–¹æ³• ====================

def fixed_size_chunking(text: str, source: str) -> list[dict]:
    """å›ºå®šå¤§å°åˆ‡å¡Š - æ¥µå¤§åƒæ•¸"""
    params = CHUNKING_PARAMS["fixed"]
    
    text_splitter = CharacterTextSplitter(
        chunk_size=params["chunk_size"],
        chunk_overlap=params["chunk_overlap"],
        separator=params["separator"],
        length_function=len
    )
    
    chunks = text_splitter.split_text(text)
    return [{"text": chunk.strip(), "source": source} 
            for chunk in chunks if len(chunk.strip()) >= 100]

def sliding_window(text: str, source: str) -> list[dict]:
    """æ»‘å‹•è¦–çª—åˆ‡å¡Š - æ¥µå¤§åƒæ•¸"""
    params = CHUNKING_PARAMS["sliding"]
    
    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        model_name="gpt-4",
        chunk_size=params["chunk_size"],
        chunk_overlap=params["chunk_overlap"],
        separators=params["separators"]
    )
    
    chunks = text_splitter.split_text(text)
    return [{"text": chunk.strip(), "source": source} 
            for chunk in chunks if len(chunk.strip()) >= 100]

def semantic_chunking(text: str, source: str) -> list[dict]:
    """èªæ„åˆ‡å¡Š - æ¥µå¤§åƒæ•¸"""
    params = CHUNKING_PARAMS["semantic"]
    
    splitter = TextSplitter((params["min_size"], params["max_size"]))
    chunks = splitter.chunks(text)
    return [{"text": chunk.strip(), "source": source} 
            for chunk in chunks if len(chunk.strip()) >= 100]

# ==================== ä¸»è™•ç†æµç¨‹ ====================

def load_data_files(data_dir: str = "./") -> list[tuple]:
    """è¼‰å…¥æ‰€æœ‰è³‡æ–™æª”æ¡ˆ"""
    print("\nğŸ“‚ è¼‰å…¥è³‡æ–™æª”æ¡ˆ...")
    
    data_files = []
    for i in range(1, 6):
        filename = f"data_0{i}.txt"
        filepath = os.path.join(data_dir, filename)
        
        if os.path.exists(filepath):
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()
            data_files.append((content, f"data_0{i}"))
            print(f"   âœ… {filename}: {len(content)} å­—å…ƒ")
        else:
            print(f"   âš ï¸ æ‰¾ä¸åˆ°: {filename}")
    
    return data_files

def process_all_methods(data_files: list[tuple]) -> dict:
    """è™•ç†æ‰€æœ‰åˆ‡å¡Šæ–¹æ³•"""
    print("\nğŸ”„ é–‹å§‹æ–‡æœ¬åˆ‡å¡Š (æ¥µå¤§åƒæ•¸)...")
    
    _, dim = get_embedding(['æ¸¬è©¦'])
    print(f"   âœ… å‘é‡ç¶­åº¦: {dim}")
    
    results = {}
    
    methods = [
        ("fixed", fixed_size_chunking, CHUNKING_PARAMS["fixed"]),
        ("sliding", sliding_window, CHUNKING_PARAMS["sliding"]),
        ("semantic", semantic_chunking, CHUNKING_PARAMS["semantic"])
    ]
    
    for method_name, chunking_func, params in methods:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è™•ç†æ–¹æ³•: {method_name}")
        print(f"   åƒæ•¸: {params}")
        print(f"{'='*60}")
        
        all_chunks = []
        for content, source in data_files:
            chunks = chunking_func(content, source)
            all_chunks.extend(chunks)
            print(f"   {source}: {len(chunks)} å€‹å€å¡Š")
        
        print(f"   ç¸½è¨ˆ: {len(all_chunks)} å€‹å€å¡Š")
        
        collection_name = f"collection_{method_name}"
        build_collection(client, collection_name, all_chunks, dim)
        results[method_name] = collection_name
    
    return results

def evaluate_questions(collections: dict, questions_df: pd.DataFrame) -> list[dict]:
    """
    è©•ä¼°æ‰€æœ‰å•é¡Œ - ä½¿ç”¨æ™ºèƒ½é¸æ“‡å–®ä¸€æœ€ä½³çµæœ
    """
    print("\nğŸ“ é–‹å§‹è©•ä¼°å•é¡Œ (æŸ¥è©¢æ“´å±• + æ™ºèƒ½é¸æ“‡)...")
    
    results = []
    record_id = 1
    total_questions = len(questions_df)
    
    for _, row in questions_df.iterrows():
        q_id = row['q_id']
        question = row['questions']
        
        print(f"\nå•é¡Œ {q_id}/{total_questions}: {question[:50]}...")
        
        # æŸ¥è©¢æ“´å±•
        expanded_queries = expand_query(question)
        print(f"   æ“´å±•æŸ¥è©¢: {len(expanded_queries)} å€‹è®Šé«”")
        
        for method_name, collection_name in collections.items():
            # å¤šæŸ¥è©¢æœå°‹
            candidates = search_multi_query(
                collection_name, 
                expanded_queries,
                top_k=SEARCH_TOP_K
            )
            
            if not candidates:
                print(f"   âš ï¸ {method_name}: ç„¡çµæœ")
                continue
            
            # ğŸ”¥ æ™ºèƒ½é¸æ“‡æœ€ä½³å€™é¸ (ä¸åˆä½µ)
            best_candidate = select_best_candidate(candidates, question)
            
            if not best_candidate:
                print(f"   âš ï¸ {method_name}: é¸æ“‡å¤±æ•—")
                continue
            
            retrieve_text = best_candidate.payload['text']
            source = best_candidate.payload['source']
            similarity = best_candidate.score
            
            # è©•åˆ†
            score = get_score(q_id, retrieve_text)
            
            results.append({
                'id': record_id,
                'q_id': q_id,
                'method': method_name,
                'retrieve_text': retrieve_text,
                'score': score,
                'source': source
            })
            
            print(f"   {method_name}: åˆ†æ•¸={score:.4f}, ç›¸ä¼¼åº¦={similarity:.4f}, é•·åº¦={len(retrieve_text)}, ä¾†æº={source}")
            record_id += 1
    
    return results

def save_results_to_csv(results: list[dict], output_file: str = "1411232095_RAG_HW_01.csv"):
    """å„²å­˜çµæœ"""
    df = pd.DataFrame(results)
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ… çµæœå·²å„²å­˜è‡³: {output_file}")
    
    print("\nğŸ“Š å„æ–¹æ³•å¹³å‡åˆ†æ•¸:")
    for method in df['method'].unique():
        method_df = df[df['method'] == method]
        avg_score = method_df['score'].mean()
        max_score = method_df['score'].max()
        min_score = method_df['score'].min()
        print(f"  {method}: å¹³å‡={avg_score:.4f}, æœ€é«˜={max_score:.4f}, æœ€ä½={min_score:.4f}")
    
    return df

def generate_report(df: pd.DataFrame):
    """ç”Ÿæˆå ±å‘Š"""
    print("\n" + "="*70)
    print("ğŸ“ˆ RAG ç³»çµ±è©•ä¼°å ±å‘Š (å–®ä¸€çµæœå„ªåŒ–ç‰ˆ)")
    print("="*70)
    
    print("\nã€1ã€‘ åƒæ•¸è¨­å®š (æ¥µå¤§åˆ‡å¡Š)")
    print("-" * 70)
    for method_name, params in CHUNKING_PARAMS.items():
        print(f"\n{method_name}:")
        for key, value in params.items():
            print(f"  - {key}: {value}")
    
    print(f"\nã€2ã€‘ å„ªåŒ–ç­–ç•¥")
    print("-" * 70)
    print(f"  - æŸ¥è©¢æ“´å±•: ç”Ÿæˆå¤šå€‹æŸ¥è©¢è®Šé«”")
    
    print("\nã€3ã€‘ è©•ä¼°çµæœ")
    print("-" * 70)
    stats = df.groupby('method')['score'].agg(['count', 'mean', 'std', 'min', 'max'])
    print(stats.to_string())
    
    print("\nã€4ã€‘ æœ€ä½³æ–¹æ³•")
    print("-" * 70)
    best_method = df.groupby('method')['score'].mean().idxmax()
    best_score = df.groupby('method')['score'].mean().max()
    print(f"æ–¹æ³•: {best_method}")
    print(f"å¹³å‡åˆ†æ•¸: {best_score:.4f}")
    
    print("\n" + "="*70)
    return best_score

# ==================== ä¸»ç¨‹å¼ ====================

def main():
    """ä¸»ç¨‹å¼"""
    print("="*70)
    print("ğŸš€ RAG ç³»çµ± - å–®ä¸€çµæœå„ªåŒ–ç‰ˆ")
    print("   ç­–ç•¥: æ¥µå¤§åˆ‡å¡Š + æ™ºèƒ½é¸æ“‡ (ä¸åˆä½µ)")
    print("="*70)
    
    try:
        data_files = load_data_files()
        if not data_files:
            print("\nâŒ ç„¡æ³•è¼‰å…¥è³‡æ–™!")
            return
        
        print("\nğŸ“‹ è¼‰å…¥å•é¡Œ...")
        questions_df = pd.read_csv("questions.csv", encoding='utf-8-sig')
        print(f"   âœ… å·²è¼‰å…¥ {len(questions_df)} å€‹å•é¡Œ")
        
        collections = process_all_methods(data_files)
        results = evaluate_questions(collections, questions_df)
        df = save_results_to_csv(results)
        generate_report(df)
        
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()