import json, os, requests, base64
from typing import Annotated, TypedDict, Literal, Dict, List, Optional
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from playwright.sync_api import sync_playwright
from langgraph.graph import StateGraph, END, add_messages
from langgraph.prebuilt import ToolNode

SEARXNG_URL = "https://puli-8080.huannago.com/search"

llm = ChatOpenAI(
    base_url="https://ws-02.wade0426.me/v1",
    api_key="",
    model="google/gemma-3-27b-it",
    temperature=0.7
)

CACHE_FILE = "query_cache.json"

def load_cache() -> Dict:
    """è¼‰å…¥å¿«å–"""
    if not os.path.exists(CACHE_FILE): return {}
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f: return json.load(f)
    except: return {}


def save_cache(cache: Dict):
    """å„²å­˜å¿«å–"""
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)


def get_cache_key(query: str) -> str:
    """ç”Ÿæˆå¿«å–éµå€¼"""
    return query.strip().lower()


class AgentState(TypedDict):
    """AI Agent çš„ç‹€æ…‹"""
    input: str  # ä½¿ç”¨è€…è¼¸å…¥çš„å•é¡Œ
    knowledge_base: List[Dict]  # å·²è’é›†çš„çŸ¥è­˜åº«
    search_queries: List[str]  # ç”Ÿæˆçš„æœå°‹é—œéµå­—
    search_results: List[Dict]  # æœå°‹çµæœ
    vlm_results: List[Dict]
    final_answer: Optional[str]  # æœ€çµ‚ç­”æ¡ˆ
    decision: str  # æ±ºç­–çµæœ ('continue' æˆ– 'finish')
    iteration: int  # ç•¶å‰è¿­ä»£æ¬¡æ•¸
    max_iterations: int  # æœ€å¤§è¿­ä»£æ¬¡æ•¸
    round_number: int # Round è¨ˆæ•¸


def check_cache_node(state: AgentState):
    """æª¢æŸ¥å¿«å–ä¸­æ˜¯å¦æœ‰ç›¸åŒå•é¡Œçš„ç­”æ¡ˆ"""
    print("\n" + "="*50)
    print(f"ğŸš€ é–‹å§‹è™•ç†å•é¡Œ: {state['input']}")
    print(f"ğŸ” [Node] æª¢æŸ¥å¿«å–: {state['input']}")
    
    cache = load_cache()
    cache_key = get_cache_key(state['input'])
    
    if cache_key in cache:
        print("âœ… å¿«å–å‘½ä¸­ï¼ç›´æ¥è¿”å›å…ˆå‰çš„ç­”æ¡ˆ")
        state['final_answer'] = cache[cache_key]['answer']
        state['decision'] = 'finish'
        state['knowledge_base'] = cache[cache_key].get('knowledge_base', [])
    else:
        print("âŒ æœªå‘½ä¸­å¿«å–ï¼Œé€²å…¥ Agent æ€è€ƒæµç¨‹ã€‚")
        state['decision'] = 'continue'
    
    return state


def planner_node(state: AgentState):
    """è¦åŠƒæŸ¥è©¢ç­–ç•¥ï¼Œåˆ¤æ–·æ˜¯å¦éœ€è¦æ›´å¤šè³‡è¨Š"""
    print("\n" + "="*50)
    print(f"âœ¨ [Think] Round {state['round_number']}")
    print("ğŸ§  [Node] Planner - è©•ä¼°ç•¶å‰çŸ¥è­˜æ˜¯å¦è¶³å¤ ...")
    
    # æª¢æŸ¥è¿­ä»£æ¬¡æ•¸
    if state['iteration'] >= state['max_iterations']:
        print(f"âš ï¸ å·²é”æœ€å¤§è¿­ä»£æ¬¡æ•¸ ({state['max_iterations']})ï¼Œå¼·åˆ¶çµæŸ")
        state['decision'] = 'finish'
        return state
    
    # æ§‹å»ºè©•ä¼°æç¤º
    prompt = f"""ä½ æ˜¯ä¸€å€‹è³‡è¨Šè©•ä¼°å°ˆå®¶ã€‚è«‹è©•ä¼°ä»¥ä¸‹æƒ…æ³ï¼š

    ä½¿ç”¨è€…å•é¡Œï¼š{state['input']}

    ç›®å‰å·²è’é›†çš„è³‡è¨Šï¼š
    {json.dumps(state['knowledge_base'], ensure_ascii=False, indent=2) if state['knowledge_base'] else 'ç›®å‰æ²’æœ‰ä»»ä½•è³‡è¨Š'}

    ä»»å‹™ï¼šåˆ¤æ–·ç›®å‰çš„è³‡è¨Šæ˜¯å¦è¶³ä»¥å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

    å›ç­”æ ¼å¼ï¼ˆåªéœ€å›ç­” YES æˆ– NOï¼‰ï¼š
    - YESï¼šå¦‚æœè³‡è¨Šå……è¶³ä¸”å¯ä»¥çµ¦å‡ºæº–ç¢ºç­”æ¡ˆ
    - NOï¼šå¦‚æœéœ€è¦æ›´å¤šè³‡è¨Šæ‰èƒ½å›ç­”

    ä½ çš„åˆ¤æ–·ï¼š"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        decision_text = response.content.strip().upper()
        
        if "YES" in decision_text:
            print("âœ… åˆ¤æ–·ï¼šè³‡è¨Šå……è¶³ï¼Œå¯ä»¥ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ")
            state['decision'] = 'finish'
        else:
            print("âŒ åˆ¤æ–·ï¼šè³‡è¨Šä¸è¶³ï¼Œéœ€è¦ç¹¼çºŒæœå°‹")
            state['decision'] = 'continue'
            state['iteration'] += 1
    except Exception as e:
        print(f"âš ï¸ Planner å‡ºéŒ¯ï¼š{e}")
        state['decision'] = 'finish'  # å‡ºéŒ¯æ™‚çµæŸæµç¨‹
    
    return state

def query_gen_node(state: AgentState) -> AgentState:
    """ç”Ÿæˆæœå°‹é—œéµå­—"""
    print("\n" + "="*50)
    print("ğŸ”‘ [Node] Query Generator - ç”Ÿæˆæœå°‹é—œéµå­—...")
    
    prompt = f"""ä½ æ˜¯ä¸€å€‹æœå°‹é—œéµå­—ç”Ÿæˆå°ˆå®¶ã€‚

    ä½¿ç”¨è€…å•é¡Œï¼š{state['input']}

    å·²æœå°‹éçš„é—œéµå­—ï¼š{state['search_queries']}

    ä»»å‹™ï¼šæ ¹æ“šä½¿ç”¨è€…å•é¡Œç”Ÿæˆ 1-2 å€‹**æ–°çš„**ç¹é«”ä¸­æ–‡æœå°‹é—œéµå­—ï¼Œé€™äº›é—œéµå­—æ‡‰è©²ï¼š
    1. èˆ‡å•é¡Œé«˜åº¦ç›¸é—œ
    2. ä¸é‡è¤‡å…ˆå‰æœå°‹éçš„é—œéµå­—
    3. èƒ½å¤ æ‰¾åˆ°æœ€æ–°ã€æœ€æº–ç¢ºçš„è³‡è¨Š

    è«‹ç›´æ¥åˆ—å‡ºé—œéµå­—ï¼Œæ¯è¡Œä¸€å€‹ï¼Œä¸è¦æœ‰å…¶ä»–èªªæ˜æ–‡å­—ã€‚

    é—œéµå­—ï¼š"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        keywords = [k.strip() for k in response.content.strip().split('\n') if k.strip()]
        keywords = [k for k in keywords if k not in state['search_queries']][:2]  # æœ€å¤š2å€‹æ–°é—œéµå­—
        
        if keywords:
            print(f"âœ… ç”Ÿæˆé—œéµå­—ï¼š{keywords}")
            state['search_queries'].extend(keywords)
        else:
            print("âš ï¸ ç„¡æ³•ç”Ÿæˆæ–°é—œéµå­—")
            state['decision'] = 'finish'
    except Exception as e:
        print(f"âš ï¸ Query Generator å‡ºéŒ¯ï¼š{e}")
        state['decision'] = 'finish'
    
    state['round_number'] += 1

    return state


def search_tool_node(state: AgentState) -> AgentState:
    """åŸ·è¡Œç¶²é æœå°‹"""
    print("\n" + "="*50)
    print("ğŸŒ [Node] Search Tool - åŸ·è¡Œæœå°‹...")
    
    if not state['search_queries']:
        print("âš ï¸ æ²’æœ‰æœå°‹é—œéµå­—")
        return state
    
    latest_query = state['search_queries'][-1]  # ä½¿ç”¨æœ€æ–°çš„é—œéµå­—
    print(f"ğŸ“ æœå°‹é—œéµå­—ï¼š{latest_query}")
    
    # å‘¼å« SearXNG
    params = {
        "q": latest_query,
        "format": "json",
        "language": "zh-TW"
    }

    try:
        response = requests.get(SEARXNG_URL, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        results = data.get('results', [])[:3]  # å–å‰3ç­†
        
        if results:
            print(f"âœ… æ‰¾åˆ° {len(results)} ç­†çµæœ")
            state['search_results'] = results

            
            
            # å°‡çµæœåŠ å…¥çŸ¥è­˜åº«
            for idx, r in enumerate(results, 1):
                state['knowledge_base'].append({
                    'source': 'search',
                    'title': r.get('title', 'ç„¡æ¨™é¡Œ'),
                    'url': r.get('url', ''),
                    'content': r.get('content', 'ç„¡æ‘˜è¦')[:300]
                })
                vlm_processor(r.get('url', ''), r.get('title', 'ç„¡æ¨™é¡Œ'), state)
                print(f"  [{idx}] {r.get('title', 'ç„¡æ¨™é¡Œ')}")

        else:
            print("âŒ æ²’æœ‰æ‰¾åˆ°æœå°‹çµæœ")
    except Exception as e:
        print(f"âš ï¸ æœå°‹å‡ºéŒ¯ï¼š{e}")
    
    return state


def vlm_processor(url: str, title: str, state: AgentState) -> AgentState:
    """
    ä½¿ç”¨ Playwright æ»¾å‹•æˆªåœ–ï¼Œä¸¦ä½¿ç”¨å¤šæ¨¡æ…‹ LLM è®€å–ç¶²é å…§å®¹ã€‚
    """
    print(f"ğŸ“¸ [VLM] å•Ÿå‹•è¦–è¦ºé–±è®€: {url}")
    
    def capture_rolling_screenshots(url, output_dir="scans_temp"):
        if not os.path.exists(output_dir): os.makedirs(output_dir)
        screenshots_b64 = []
        
        try:
            with sync_playwright() as p:
                # å•Ÿå‹•ç€è¦½å™¨ (Headless æ¨¡å¼)
                browser = p.chromium.launch(
                    headless=True, 
                    args=["--disable-blink-features=AutomationControlled"] # è¦é¿éƒ¨åˆ†åçˆ¬èŸ²
                )
                
                # è¨­å®š viewport (æ¨¡æ“¬æ¡Œé¢ç€è¦½)
                context = browser.new_context(viewport={'width': 1280, 'height': 1200})
                page = context.new_page()
                
                # å‰å¾€ç¶²é 
                page.goto(url, wait_until="domcontentloaded", timeout=30000)
                page.wait_for_timeout(3000) # ç­‰å¾…æ¸²æŸ“
                
                # --- CSS Injection (å»å»£å‘Š/å½ˆçª—) ---
                page.add_style_tag(content="""
                    iframe { opacity: 0 !important; pointer-events: none !important; }
                    div[id*='cookie'], div[class*='cookie'], div[id*='ads'], div[class*='ads'] { display: none !important; }
                    div[class*='overlay'], div[id*='overlay'], div[class*='popup'] { opacity: 0 !important; pointer-events: none !important; }
                    header, nav { position: absolute !important; } /* é˜²æ­¢ sticky header é®æ“‹æˆªåœ– */
                """)

                total_height = page.evaluate("document.body.scrollHeight")
                viewport_height = 1200
                current_scroll = 0
                
                for i in range(3):
                    # æ»¾å‹•
                    page.evaluate(f"window.scrollTo(0, {current_scroll})")
                    page.wait_for_timeout(1000) # ç­‰å¾…æ»¾å‹•å¾Œæ¸²æŸ“
                    
                    # æˆªåœ–ä¸¦è½‰ Base64
                    b64 = base64.b64encode(page.screenshot()).decode('utf-8')
                    screenshots_b64.append(b64)
                    print(f"   - æˆªåœ– {i+1} å®Œæˆ (Scroll: {current_scroll})")
                    
                    current_scroll += (viewport_height - 200) # é‡ç–Š 200px é¿å…å‰²è£‚æ–‡å­—
                    if current_scroll >= total_height: break
                    
                browser.close()
        except Exception as e:
            print(f"âŒ æˆªåœ–å¤±æ•—: {e}")
        
        state["vlm_results"] = screenshots_b64
        return state

    # åŸ·è¡Œæˆªåœ–
    images = capture_rolling_screenshots(url)
    
    if not images: 
        return "éŒ¯èª¤ï¼šç„¡æ³•è®€å–ç¶²é å…§å®¹æˆ–æˆªåœ–å¤±æ•—ã€‚"

    print(f"ğŸ¤– [LLM] æ­£åœ¨åˆ†æ {len(images)} å¼µåœ–ç‰‡...")

    # --- çµ„è£å¤šæ¨¡æ…‹è¨Šæ¯ ---
    msg_content = [
        {
            "type": "text", 
            "text": f"é€™æ˜¯ä¸€å€‹ç¶²é çš„æ»¾å‹•æˆªåœ–ï¼Œæ¨™é¡Œç‚ºï¼š{title}ã€‚\nè«‹å¿½ç•¥å»£å‘Šèˆ‡å°èˆªæ¬„ï¼Œæ‘˜è¦æ­¤ç¶²é çš„æ ¸å¿ƒå…§å®¹ï¼Œä¸¦ç‰¹åˆ¥é—œæ³¨ä»»ä½•æ•¸æ“šã€æ—¥æœŸæˆ–å…·é«”äº‹å¯¦ã€‚"
        }
    ]
    
    # åŠ å…¥æ‰€æœ‰åœ–ç‰‡
    for img in images:
        msg_content.append({
            "type": "image_url", 
            "image_url": {"url": f"data:image/png;base64,{img}"}
        })
    
    # å‘¼å« LLM
    try:
        response = llm.invoke([HumanMessage(content=msg_content)])
        return response.content
    except Exception as e:
        return f"LLM åˆ†æå¤±æ•—: {e}"


def final_answer_node(state: AgentState) -> AgentState:
    """ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ"""
    print("\n" + "="*50)
    print("ğŸ“ [Node] Final Answer - ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ...")
    
    if not state['knowledge_base']:
        state['final_answer'] = "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•æ‰¾åˆ°è¶³å¤ çš„è³‡è¨Šä¾†å›ç­”æ‚¨çš„å•é¡Œã€‚"
        return state
    
    # æ§‹å»ºç­”æ¡ˆç”Ÿæˆæç¤º
    knowledge_summary = "\n\n".join([
        f"ä¾†æº {idx+1}ï¼š{item['title']}\n{item['content']}"
        for idx, item in enumerate(state['knowledge_base'])
    ])
    
    prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è³‡è¨Šæ•´åˆåŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

    ä½¿ç”¨è€…å•é¡Œï¼š{state['input']}

    åƒè€ƒè³‡æ–™ï¼š
    {knowledge_summary}

    è¦æ±‚ï¼š
    1. æ ¹æ“šåƒè€ƒè³‡æ–™æä¾›æº–ç¢ºã€å®Œæ•´çš„ç­”æ¡ˆ
    2. å¦‚æœè³‡æ–™ä¸­æœ‰çŸ›ç›¾ï¼Œè«‹æŒ‡å‡ºä¸¦èªªæ˜
    3. ç­”æ¡ˆè¦æ¸…æ™°ã€æœ‰æ¢ç†
    4. å¦‚æœè³‡æ–™ä¸è¶³ï¼Œè«‹èª å¯¦èªªæ˜

    ä½ çš„ç­”æ¡ˆï¼š"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        state['final_answer'] = response.content.strip()
        print("âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
        
        # å„²å­˜åˆ°å¿«å–
        cache = load_cache()
        cache_key = get_cache_key(state['input'])
        cache[cache_key] = {
            'answer': state['final_answer'],
            'knowledge_base': state['knowledge_base']
        }
        save_cache(cache)
        print("ğŸ’¾ å·²å„²å­˜åˆ°å¿«å–")
    except Exception as e:
        print(f"âš ï¸ ç­”æ¡ˆç”Ÿæˆå‡ºéŒ¯ï¼š{e}")
        state['final_answer'] = "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    
    return state


def route_after_cache(state):
    return "planner" if state['decision'] == 'continue' else "final_answer"


def route_after_planner(state):
    return "query_gen" if state['decision'] == 'continue' else "final_answer"


workflow = StateGraph(AgentState)

workflow.add_node("check_cache", check_cache_node)
workflow.add_node("planner", planner_node)
workflow.add_node("query_gen", query_gen_node)
workflow.add_node("search_tool", search_tool_node)
workflow.add_node("final_answer", final_answer_node)

workflow.set_entry_point("check_cache")

workflow.add_conditional_edges(
    "check_cache",
    route_after_cache,
    {
        "planner": "planner",
        "final_answer": "final_answer"
    }
)

workflow.add_conditional_edges(
    "planner",
    route_after_planner,
    {
        "query_gen": "query_gen",
        "final_answer": "final_answer"
    }
)

workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "planner")  # æœå°‹å¾Œå›åˆ° planner è©•ä¼°
workflow.add_edge("final_answer", END)

app = workflow.compile()
print(app.get_graph().draw_ascii())

if __name__ == "__main__":
    while True:
        user_input = input("\nè«‹è¼¸å…¥å•é¡Œ: ")
        if user_input.lower() in ["exit", "q"]: break
        inputs = {
            "input": user_input,
            "knowledge_base": [],
            "search_queries": [],
            "search_results": [],
            "vlm_results": [],
            "final_answer": None,
            "decision": "continue",
            "iteration": 0,
            "max_iterations": 3,
            "round_number": 0
        }
        result = app.invoke(inputs)

        # è¼¸å‡ºçµæœ
        print("\n" + "="*60)
        print("âœ¨ æŸ¥è­‰å®Œæˆï¼")
        print("="*60)
        print(f"\nã€æœ€çµ‚ç­”æ¡ˆã€‘\n{result['final_answer']}")
        print(f"\nã€å…±ä½¿ç”¨ {len(result['knowledge_base'])} å€‹è³‡æ–™ä¾†æºã€‘")
        print(f"ã€åŸ·è¡Œäº† {result['iteration']} æ¬¡è¿­ä»£ã€‘")